<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link href="./index.css" rel="stylesheet" />
  <title>Single Layer Perceptron Visualization</title>
</head>
<body>
  <header>
    <h1>Visualizing the Learning Process of a Single Layer Perceptron</h1>
    <p class="subtitle">Explore how neural networks minimize error to learn accurate models</p>
  </header>

  <section class="intro">
    <h2>Input Data & True Relationship</h2>
    <p>The actual function that generates the data is <strong>y = 2x + 1</strong>. But to simulate real-world scenarios, we add some noise to the output values.</p>
    <table class="sample-data">
      <thead>
        <tr><th>x</th><th>y (with noise)</th></tr>
      </thead>
      <tbody>
        <tr><td>-1.0</td><td>-0.99</td></tr>
        <tr><td>0.0</td><td>1.01</td></tr>
        <tr><td>1.5</td><td>4.03</td></tr>
        <tr><td>2.0</td><td>5.00</td></tr>
        <tr><td>-0.7</td><td>-0.39</td></tr>
      </tbody>
    </table>
  </section>

  <div class="comment-box">
    In this demo, we define a simple Perceptron model that attempts to learn the true equation from the data. The learning process involves minimizing the prediction error.
  </div>

  <div class="diagram-container">
    <div class="diagram">
      <div class="circle">x₁</div>
      <div class="arrow">→</div>
      <div class="box">w₁x₁ + b</div>
      <div class="arrow">→</div>
      <div class="circle">y</div>
    </div>
  </div>

  <h3>Prediction Equation</h3>
  <div class="equations">
    <p class="math">yᵖ = w₁x₁ + b</p>
  </div>

  <div class="comment-box">
    Our goal is to find optimal values of <strong>w₁</strong> (weight) and <strong>b</strong> (bias) such that the model's predictions align with actual data. To do this, we define a loss function and minimize it.
  </div>

  <h3>Error Function: Mean Squared Error (MSE)</h3>
  <div class="equations">
    <p class="math">J(w₁, b) = (1/N) ∑ (yᵢ - (w₁xᵢ + b))²</p>
    <p class="math">
      J(w₁, b) = (1/5) × [ (0.99 − w₁(−1) − b)² + (1.01 − w₁(0) − b)² + (4.03 − w₁(1.5) − b)² + (5 − w₁(2) − b)² + (−0.39 − w₁(−0.7) − b)² ]
    </p>
  </div>

  <div class="container">
    <div class="card">
      <h2>Model Equation Visualization</h2>
      <div class="iframe-container">
        <iframe src="https://www.geogebra.org/calculator/hyhjrmx9?embed" allowfullscreen></iframe>
      </div>
    </div>
    <div class="card">
      <h2>Error Surface Visualization</h2>
      <div class="iframe-container">
        <iframe src="https://www.geogebra.org/calculator/dfb84kvt?embed" allowfullscreen></iframe>
      </div>
    </div>
  </div>

  <div class="comment-box">
    The surface plotted above shows how error varies with different weights and biases. The lowest point on this surface (minima) gives the optimal values of w₁ and b. For this data, the error is minimized at (w=2, b=1), which closely matches the true equation.
  </div>

  <footer>
    Designed for intuitive understanding of how a Single Layer Perceptron learns using error minimization.
  </footer>
</body>
</html>
